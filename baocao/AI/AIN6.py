from flask import Flask, request, jsonify, send_from_directory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
import os
import random
import difflib
import platform
import psutil
import socket
import getpass
import requests
from bs4 import BeautifulSoup
import numpy as np

import greeting
import creator
import bye
import story
import math_logic

app = Flask(__name__)

# ------------------ H√†m x·ª≠ l√Ω vƒÉn b·∫£n ------------------
def clean_text(text):
    return text.lower().strip()
def detect_topic(cleaned_input):
    for topic, keywords in topic_keywords.items():
        for kw in keywords:
            if kw in cleaned_input:
                return topic
    return None
# ------------------ D·ªØ li·ªáu offline ------------------
training_sentences = []
training_labels = []

# greeting
training_sentences += [clean_text(s) for s in greeting.training_sentences]
training_labels += ["greeting"] * len(greeting.training_sentences)

# story
training_sentences += [clean_text(s) for s in story.training_sentences]
training_labels += ["story"] * len(story.training_sentences)

# creator
training_sentences += [clean_text(s) for s in creator.training_sentences]
training_labels += ["creator"] * len(creator.training_sentences)

# bye
training_sentences += [clean_text(s) for s in bye.training_sentences]
training_labels += ["bye"] * len(bye.training_sentences)

responses = {}
responses.update(greeting.responses)
responses.update(story.responses)
responses.update(creator.responses)
responses.update(bye.responses)

# ------------------ Log c√¢u ch∆∞a h·ªçc ------------------
def log_unlearned(sentence: str, label="unknown"):
    sentence = sentence.strip()
    if not sentence:
        return
    file_path = "unlearned.txt"
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("DANH S√ÅCH C√ÇU CH∆ØA H·ªåC:\n")
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.read().splitlines()
    if not any(sentence in line for line in lines):
        with open(file_path, "a", encoding="utf-8") as f:
            f.write(f"{sentence}||{label}\n")
        print(f"üìù ƒê√£ l∆∞u c√¢u m·ªõi v√†o unlearned.txt: {sentence}||{label}")

# ------------------ N·∫°p c√¢u ch∆∞a h·ªçc ------------------
def load_unlearned():
    file_path = "unlearned.txt"
    new_sentences = []
    new_labels = []
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.read().splitlines()
        for line in lines[1:]:
            if "||" in line:
                sentence, label = line.split("||", 1)
                new_sentences.append(clean_text(sentence))
                new_labels.append(label)
    return new_sentences, new_labels

# ------------------ Fuzzy match ------------------
def fuzzy_match(sentence, choices, cutoff=0.6):
    matches = difflib.get_close_matches(sentence, choices, n=1, cutoff=cutoff)
    return matches[0] if matches else None

# ------------------ Th√¥ng tin h·ªá th·ªëng ------------------
def system_info():
    info = {}
    info['OS'] = f"{platform.system()} {platform.release()} ({platform.version()})"
    info['Architecture'] = platform.machine()
    info['Processor'] = platform.processor()
    info['CPU Cores'] = psutil.cpu_count(logical=False)
    info['Logical CPUs'] = psutil.cpu_count(logical=True)
    info['RAM'] = f"{round(psutil.virtual_memory().total / (1024**3), 2)} GB"
    info['Hostname'] = socket.gethostname()
    info['IP Address'] = socket.gethostbyname(socket.gethostname())
    info['User'] = getpass.getuser()
    return info

# ------------------ Load c√¢u ch∆∞a h·ªçc v√† hu·∫•n luy·ªán offline ------------------
new_sentences, new_labels = load_unlearned()
training_sentences += new_sentences
training_labels += new_labels

# ƒê·ªìng b·ªô s·ªë l∆∞·ª£ng
min_len = min(len(training_sentences), len(training_labels))
training_sentences = training_sentences[:min_len]
training_labels = training_labels[:min_len]

# Hu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes offline
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(training_sentences)
model = MultinomialNB()
model.fit(X, training_labels)

print(f"‚úÖ M√¥ h√¨nh offline ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi {len(training_sentences)} c√¢u, bao g·ªìm {len(new_sentences)} c√¢u ch∆∞a h·ªçc")

# ------------------ Fetch website theo topic ------------------
topic_keywords = {
    "ai": ["ai", "tr√≠ tu·ªá nh√¢n t·∫°o", "artificial intelligence"],
    "math": ["to√°n", "to√°n h·ªçc", "math", "mathematics"],
    "history": ["l·ªãch s·ª≠", "history", "historical"],
    "sports": ["th·ªÉ thao", "b√≥ng ƒë√°", "sports", "football"],
    "world_news": ["tin t·ª©c", "th·∫ø gi·ªõi", "news", "world news"]
}
topic_urls = {
    "ai": ["https://en.wikipedia.org/wiki/Artificial_intelligence"],
    "math": ["https://en.wikipedia.org/wiki/Mathematics"],
    "history": ["https://en.wikipedia.org/wiki/History"],
    "sports": ["https://en.wikipedia.org/wiki/Sport"],
    "world_news": ["https://en.wikipedia.org/wiki/World_news"]
}
topic_chunks = {}       # l∆∞u c√°c ƒëo·∫°n vƒÉn theo topic
topic_vectorizers = {}  # vectorizer ri√™ng cho m·ªói topic
topic_X = {}            # ma tr·∫≠n TF-IDF cho m·ªói topic

def fetch_website_text(url):
    try:
        res = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.text, "html.parser")
        text = soup.get_text(separator=" ")
        words = text.split()
        chunk_size = 150
        chunks = [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]
        return chunks
    except Exception as e:
        print("‚ùå L·ªói khi ƒë·ªçc website:", e)
        return []

# Fetch t·ª´ng topic
for topic, urls in topic_urls.items():
    all_chunks = []
    for url in urls:
        chunks = fetch_website_text(url)
        all_chunks += chunks
    topic_chunks[topic] = all_chunks
    vectorizer_topic = TfidfVectorizer()
    if all_chunks:
        X_topic = vectorizer_topic.fit_transform(all_chunks)
    else:
        X_topic = None
    topic_vectorizers[topic] = vectorizer_topic
    topic_X[topic] = X_topic
    print(f"[INFO] Topic '{topic}' n·∫°p {len(all_chunks)} chunks.")

# ------------------ Cosine similarity ------------------
def cosine_similarity(a, b):
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def answer_from_online_topics(question):
    cleaned = clean_text(question)
    topic = detect_topic(cleaned)

    if topic is None:
        return "M√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c th√¥ng tin t·ª´ online."

    chunks = topic_chunks.get(topic, [])
    X_topic = topic_X.get(topic, None)
    vectorizer_topic = topic_vectorizers.get(topic, None)

    if not chunks or X_topic is None:
        return "M√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c th√¥ng tin t·ª´ online."

    q_vec = vectorizer_topic.transform([question])
    scores = (X_topic @ q_vec.T).toarray().ravel()

    idx = np.argmax(scores)
    best_score = scores[idx]

    if best_score < 0.1:
        return "M√¨nh ch∆∞a t√¨m ƒë∆∞·ª£c th√¥ng tin t·ª´ online."

    return chunks[idx]


# ------------------ Routes ------------------
@app.route("/")
def home():
    web_dir = r"D:\git\AI_chat_box\baocao\AI\web"
    return send_from_directory(web_dir, "index.html")

@app.route("/chat", methods=["POST"])
def chat():
    data = request.get_json()
    user_input = clean_text(data.get("message", ""))
    reply = "M√¨nh ch∆∞a hi·ªÉu c√¢u n√†y."  # default fallback

    # Ki·ªÉm tra to√°n h·ªçc
    math_result = math_logic.try_math(user_input)
    if math_result:
        reply = math_result
        return jsonify({"response": reply})

    # Ki·ªÉm tra th√¥ng tin h·ªá th·ªëng
    if any(keyword in user_input for keyword in ["th√¥ng tin m√°y", "c·∫•u h√¨nh", "system info", "m√°y"]):
        info = system_info()
        reply = "\n".join([f"{k}: {v}" for k, v in info.items()])
        return jsonify({"response": reply})

    # --- Offline: Fuzzy match ---
    match = fuzzy_match(user_input, training_sentences)
    if match:
        idx = training_sentences.index(match)
        predicted_label = training_labels[idx]
        reply = random.choice(responses.get(predicted_label, [reply]))
    else:
        # D√πng Naive Bayes d·ª± ƒëo√°n
        X_test = vectorizer.transform([user_input])
        probs = model.predict_proba(X_test)[0]
        max_prob = max(probs)
        predicted_label = model.classes_[probs.argmax()]

        if max_prob < 0.5:
            # fallback: t√¨m trong online theo topic
            log_unlearned(user_input, label="unknown")
            reply = answer_from_online_topics(user_input)
            log_unlearned(user_input, label="online")
        else:
            reply = random.choice(responses.get(predicted_label, [reply]))

    print(f"[DEBUG] input: {user_input}, label: {predicted_label}")
    return jsonify({"response": reply})

# ------------------ Ch·∫°y server ------------------
if __name__ == "__main__":
    print("Chatbot AAIN6 ƒëang ho·∫°t ƒë·ªông!")
    print("üåê M·ªü tr√¨nh duy·ªát v√† v√†o: http://127.0.0.1:5000")
    app.run(port=5000)
